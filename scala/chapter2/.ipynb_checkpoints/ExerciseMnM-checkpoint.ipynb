{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26014608-5175-454d-92e2-a9084bb2a105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://725a4c8e5c14:4042\n",
       "SparkContext available as 'sc' (version = 3.2.1, master = local[*], app id = local-1648431401167)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\n",
       "mNmDf: org.apache.spark.sql.DataFrame = [State: string, Color: string ... 1 more field]\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "val mNmDf = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"../../resources/chapter2/mnm_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95df4486-4e54-4301-88a5-fd7e1aca6687",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----+\n",
      "|State| Color|Total|\n",
      "+-----+------+-----+\n",
      "|   AZ|  Blue| 1636|\n",
      "|   AZ| Brown| 1698|\n",
      "|   AZ| Green| 1676|\n",
      "|   AZ|Orange| 1689|\n",
      "|   AZ|   Red| 1648|\n",
      "|   AZ|Yellow| 1654|\n",
      "|   CA|  Blue| 1603|\n",
      "|   CA| Brown| 1718|\n",
      "|   CA| Green| 1723|\n",
      "|   CA|Orange| 1657|\n",
      "|   CA|   Red| 1656|\n",
      "|   CA|Yellow| 1807|\n",
      "|   CO|  Blue| 1695|\n",
      "|   CO| Brown| 1656|\n",
      "|   CO| Green| 1713|\n",
      "|   CO|Orange| 1642|\n",
      "|   CO|   Red| 1624|\n",
      "|   CO|Yellow| 1721|\n",
      "|   NM|  Blue| 1638|\n",
      "|   NM| Brown| 1687|\n",
      "+-----+------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----+------+-----+\n",
      "|State| Color|Total|\n",
      "+-----+------+-----+\n",
      "|   CA|  Blue| 1603|\n",
      "|   CA| Brown| 1718|\n",
      "|   CA| Green| 1723|\n",
      "|   CA|Orange| 1657|\n",
      "|   CA|   Red| 1656|\n",
      "|   CA|Yellow| 1807|\n",
      "|   NV|  Blue| 1673|\n",
      "|   NV| Brown| 1657|\n",
      "|   NV| Green| 1698|\n",
      "|   NV|Orange| 1712|\n",
      "|   NV|   Red| 1610|\n",
      "|   NV|Yellow| 1675|\n",
      "|   TX|  Blue| 1614|\n",
      "|   TX| Brown| 1641|\n",
      "|   TX| Green| 1737|\n",
      "|   TX|Orange| 1652|\n",
      "|   TX|   Red| 1725|\n",
      "|   TX|Yellow| 1703|\n",
      "+-----+------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "countAggMnM: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [State: string, Color: string ... 1 more field]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/*\n",
    "-Group by state and color and count colors\n",
    "-Show only results of California,Texas and Nevada\n",
    "*/\n",
    "val countAggMnM = mNmDf\n",
    "        .select($\"State\",$\"Color\",$\"Count\")\n",
    "        .groupBy($\"State\",$\"Color\")\n",
    "        .agg(count($\"Count\").as(\"Total\"))\n",
    "        .orderBy($\"State\",$\"Color\")\n",
    "        .cache\n",
    "\n",
    "countAggMnM.show\n",
    "\n",
    "countAggMnM.filter($\"State\".equalTo(\"CA\") || $\"State\".equalTo(\"NV\") || $\"State\".equalTo(\"TX\"))\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4cc800a-4570-4a01-aa30-9edb255c09c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+\n",
      "|State| Color|Max Count|\n",
      "+-----+------+---------+\n",
      "|   AZ| Brown|      100|\n",
      "|   AZ|  Blue|      100|\n",
      "|   AZ|Orange|      100|\n",
      "|   AZ|   Red|      100|\n",
      "|   AZ| Green|      100|\n",
      "|   AZ|Yellow|      100|\n",
      "|   CA| Green|      100|\n",
      "|   CA|   Red|      100|\n",
      "|   CA| Brown|      100|\n",
      "|   CA|  Blue|      100|\n",
      "|   CA|Yellow|      100|\n",
      "|   CA|Orange|      100|\n",
      "|   CO|Yellow|      100|\n",
      "|   CO|   Red|      100|\n",
      "|   CO| Brown|      100|\n",
      "|   CO| Green|      100|\n",
      "|   CO|  Blue|      100|\n",
      "|   CO|Orange|      100|\n",
      "|   NM| Green|      100|\n",
      "|   NM|Orange|      100|\n",
      "+-----+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "maxAggMnM: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [State: string, Color: string ... 1 more field]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Group by state and color and aggregate max color\n",
    "val maxAggMnM = mNmDf\n",
    "  .select($\"State\",$\"Color\",$\"Count\")\n",
    "  .groupBy($\"State\",$\"Color\")\n",
    "  .agg(max(\"Count\").as(\"Max Count\"))\n",
    "  .orderBy($\"State\")\n",
    "maxAggMnM.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1bd6fbc-333d-4a1d-a2b9-d4c8c6d1e627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----+---+---+-----+\n",
      "|State| Color|Count|Max|Min|  Avg|\n",
      "+-----+------+-----+---+---+-----+\n",
      "|   WY| Green| 1695|100| 10|55.66|\n",
      "|   NV|   Red| 1610|100| 10|55.49|\n",
      "|   UT|  Blue| 1655|100| 10|54.37|\n",
      "|   WA|Orange| 1658|100| 10| 55.2|\n",
      "|   NM| Green| 1682|100| 10| 54.2|\n",
      "|   CA|  Blue| 1603|100| 10| 55.6|\n",
      "|   WA|   Red| 1671|100| 10|55.85|\n",
      "|   NV| Brown| 1657|100| 10|55.81|\n",
      "|   AZ| Green| 1676|100| 10|54.82|\n",
      "|   CA|   Red| 1656|100| 10|55.27|\n",
      "|   AZ|Orange| 1689|100| 10|54.28|\n",
      "|   CO|  Blue| 1695|100| 10|55.11|\n",
      "|   NM|Orange| 1665|100| 10|54.81|\n",
      "|   NM|Yellow| 1688|100| 10|54.94|\n",
      "|   WY|Orange| 1595|100| 10|55.14|\n",
      "|   UT|Yellow| 1645|100| 10|54.26|\n",
      "|   WY|   Red| 1670|100| 10|54.95|\n",
      "|   OR|  Blue| 1646|100| 10| 55.0|\n",
      "|   NV|Orange| 1712|100| 10|54.87|\n",
      "|   AZ|Yellow| 1654|100| 10|54.99|\n",
      "+-----+------+-----+---+---+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "someAggMnM: org.apache.spark.sql.DataFrame = [State: string, Color: string ... 4 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Group by state and color and aggregate some aggregations color\n",
    "val someAggMnM = mNmDf\n",
    "  .select($\"State\",$\"Color\",$\"Count\")\n",
    "  .groupBy($\"State\",$\"Color\")\n",
    "  .agg(count($\"Count\").as(\"Count\")\n",
    "    ,max($\"Count\").as(\"Max\")\n",
    "    ,min($\"Count\").as(\"Min\")\n",
    "    ,(round(avg($\"Count\")*100)/100).as(\"Avg\"))\n",
    "\n",
    "someAggMnM.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
